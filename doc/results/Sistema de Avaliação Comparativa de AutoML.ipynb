{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee789f90",
   "metadata": {},
   "source": [
    "# M√©todologia de Avalia√ß√£o de Ferramentas AutoML\n",
    "\n",
    "## üìä Sistema de Pontua√ß√£o\n",
    "\n",
    "O modelo utiliza uma abordagem quantitativa padronizada para avaliar ferramentas de AutoML em cinco dimens√µes cr√≠ticas:\n",
    "\n",
    "1. **Descri√ß√£o Funcional**\n",
    "2. **An√°lise Estat√≠stica** \n",
    "3. **Transpar√™ncia Algor√≠tmica**\n",
    "4. **Interpretabilidade**\n",
    "5. **An√°lise Interna**\n",
    "\n",
    "### üî¢ M√©trica de Avalia√ß√£o\n",
    "\n",
    "$$\n",
    "S = \\begin{cases} \n",
    "0 & \\text{if } N = 0 \\\\\n",
    "1 & \\text{if } N = 1 \\\\\n",
    "2 & \\text{if } N \\geq 2 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Para cada crit√©rio de avalia√ß√£o, √© atribu√≠do um valor **N** que representa o n√∫mero de maneiras como a ferramenta atende ao requisito:\n",
    "\n",
    "| Avalia√ß√£o | Pontua√ß√£o (P·µ¢) | Significado |\n",
    "|-----------|----------------|-------------|\n",
    "| N = 0     | 0 pontos       | N√£o atende ao crit√©rio |\n",
    "| N = 1     | 1 ponto        | Atendimento parcial |\n",
    "| N ‚â• 2     | 2 pontos       | Atendimento total |\n",
    "\n",
    "### üìà F√≥rmula de Normaliza√ß√£o\n",
    "\n",
    "O score normalizado por categoria √© calculado como:\n",
    "\n",
    "$$\n",
    "S = \\left( \\frac{\\sum_{i=1}^{N} P_i}{N \\cdot W} \\right) \\times 100\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "- **S**: Score normalizado (0-100%)\n",
    "- **P·µ¢**: Pontua√ß√£o da i-√©sima pergunta (0, 1 ou 2)\n",
    "- **N**: N√∫mero total de perguntas na categoria\n",
    "- **W**: Pontua√ß√£o m√°xima por pergunta (W=2)\n",
    "\n",
    "## üñ•Ô∏è Como Usar o Modelo\n",
    "\n",
    "1. **Sele√ß√£o da Ferramenta**\n",
    "   - Escolha a ferramenta no menu suspenso\n",
    "   - Para novas ferramentas, digite o nome no campo \"Outra\"\n",
    "\n",
    "2. **Processo de Avalia√ß√£o**\n",
    "   ```python\n",
    "   Clique em \"Iniciar Nova Avalia√ß√£o\"\n",
    "   Para cada pergunta:\n",
    "   - Selecione N=0 (N√£o atende)\n",
    "   - Selecione N=1 (Parcial)\n",
    "   - Selecione N‚â•2 (Total)\n",
    "   Clique em \"Calcular Scores\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436d0470",
   "metadata": {},
   "source": [
    "# üìä Visualiza√ß√£o de Resultados\n",
    "\n",
    "## Tabelas\n",
    "- **Tabelas detalhadas** por crit√©rio\n",
    "- **Gr√°ficos comparativos** autom√°ticos\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Sa√≠das do Modelo\n",
    "\n",
    "### 1. Tabela Resumo\n",
    "Mostra por categoria:\n",
    "- Score normalizado (%)\n",
    "- Pontua√ß√£o total\n",
    "- N√∫mero de perguntas\n",
    "\n",
    "### 2. Tabela Detalhada\n",
    "Apresenta todas as avalia√ß√µes individuais com:\n",
    "- Crit√©rio avaliado\n",
    "- Valor de N atribu√≠do\n",
    "- Pontua√ß√£o calculada\n",
    "- Score da categoria\n",
    "\n",
    "### 3. Visualiza√ß√µes Gr√°ficas\n",
    "\n",
    "| Tipo de Gr√°fico | Utilidade | Exemplo |\n",
    "|-----------------|-----------|---------|\n",
    "| ![Barras](https://matplotlib.org/stable/_images/sphx_glr_bar_001.png) | Compara√ß√£o direta entre categorias | `plt.bar()` |\n",
    "| ![Radar](https://matplotlib.org/stable/_images/sphx_glr_radar_chart_001.png) | Vis√£o hol√≠stica do desempenho | `plt.polar()` |\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Considera√ß√µes Metodol√≥gicas\n",
    "\n",
    "‚úî **Objetividade**: Pontua√ß√£o baseada em crit√©rios mensur√°veis  \n",
    "‚úî **Normaliza√ß√£o**: Compara√ß√£o entre categorias com diferentes n√∫meros de crit√©rios  \n",
    "‚úî **Reprodutibilidade**: Aplic√°vel a qualquer ferramenta AutoML  \n",
    "‚úî **Abordagem Quantitativa**: Reduz subjetividade na avalia√ß√£o  \n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Fluxo de Trabalho Recomendado\n",
    "\n",
    "1. Avalie todas as ferramentas de interesse\n",
    "2. Consulte a an√°lise comparativa\n",
    "3. Identifique pontos fortes e fracos\n",
    "4. Repita para novas vers√µes\n",
    "\n",
    "> **Nota**: Os resultados s√£o salvos automaticamente permitindo:\n",
    "> - An√°lises temporais\n",
    "> - Consolida√ß√£o de dados\n",
    "> - Compara√ß√£o hist√≥rica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6a849e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "// Mant√©m os outputs vis√≠veis\n",
       "Jupyter.notebook.get_cells().forEach(function(cell) {\n",
       "    if (cell.cell_type === 'code') {\n",
       "        cell.output_area.outputs = cell.output_area.outputs || [];\n",
       "    }\n",
       "});\n",
       "\n",
       "import ipywidgets as widgets\n",
       "from IPython.display import display, clear_output\n",
       "import pandas as pd\n",
       "import matplotlib.pyplot as plt\n",
       "import numpy as np\n",
       "from math import pi\n",
       "import pickle\n",
       "import os\n",
       "\n",
       "# Configura√ß√µes de estilo\n",
       "plt.style.use('seaborn')\n",
       "primary_color = '#1f77b4'\n",
       "palette = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
       "\n",
       "## Dados das Perguntas por Categoria\n",
       "questions = {\n",
       "    \"Descri√ß√£o Funcional\": [\n",
       "        \"√â poss√≠vel identificar quais s√£o as etapas do pipeline?\",\n",
       "        \"√â poss√≠vel identificar os componentes utilizados em cada etapa?\",\n",
       "        \"Est√° claro quais s√£o as entradas e sa√≠das?\",\n",
       "        \"Est√° claro como o processo √© executado na pr√°tica?\"\n",
       "    ],\n",
       "    \"An√°lise Estat√≠stica\": [\n",
       "        \"As previs√µes do modelo s√£o apresentadas para as classes (malwares e benignos)?\",\n",
       "        \"Mantem o hist√≥rico dos dados utilizados para treinar e testar o modelo?\",\n",
       "        \"Na sele√ß√£o de caracter√≠sticas √© informado ao usuario quais s√£o as mais relevantes?\",\n",
       "        \"As m√©tricas de desempenho do modelo s√£o apresentadas?\",\n",
       "        \"Existem formas de visualizar os resultados (gr√°ficos, tabelas)?\"\n",
       "    ],\n",
       "    \"Transpar√™ncia Algor√≠tmica\": [\n",
       "        \"√â poss√≠vel identificar quais modelos s√£o utilizados?\",\n",
       "        \"√â possivel identificar avisos, informa√ß√µes e erros registrados nos logs do sistema?\",\n",
       "        \"√â poss√≠vel identificar os hiperparametros dos modelos gerados?\",\n",
       "        \"√â poss√≠vel identificar se os dados s√£o balanceados?\"\n",
       "    ],\n",
       "    \"Interpretabilidade\": [\n",
       "        \"O motivo da redu√ß√£o da dimensionalidade √© explicado? (Pr√©-Modelo)\",\n",
       "        \"Existem t√©cnicas espec√≠ficas de redu√ß√£o de dimensionalidade para o dom√≠nio? (Pr√©-Modelo)\",\n",
       "        \"H√° m√©todos de interpretabilidade global dispon√≠veis?\",\n",
       "        \"O framework oferece modelos com interpretabilidade intr√≠nseca? (In-Modelo)\",\n",
       "        \"√â poss√≠vel avaliar a diferen√ßa entre previs√µes e valores reais?\",\n",
       "        \"H√° m√©todos de interpretabilidade independentes do modelo? (P√≥s-Modelo)\"\n",
       "    ],\n",
       "    \"An√°lise Interna\": [\n",
       "        \"Existem m√©todos para visualizar a estrutura do modelo? (P√≥s-Modelo)\"\n",
       "    ]\n",
       "}\n",
       "\n",
       "# Arquivo para salvar os resultados\n",
       "RESULTS_FILE = 'automl_evaluations.pkl'\n",
       "\n",
       "class AutoMLComparativeEvaluation:\n",
       "    def __init__(self):\n",
       "        self.all_results = pd.DataFrame()\n",
       "        self.load_existing_data()\n",
       "    \n",
       "    def load_existing_data(self):\n",
       "        if os.path.exists(RESULTS_FILE):\n",
       "            with open(RESULTS_FILE, 'rb') as f:\n",
       "                self.all_results = pickle.load(f)\n",
       "    \n",
       "    def save_results(self):\n",
       "        with open(RESULTS_FILE, 'wb') as f:\n",
       "            pickle.dump(self.all_results, f)\n",
       "    \n",
       "    def add_evaluation(self, tool_name, results_df):\n",
       "        results_df['Ferramenta'] = tool_name\n",
       "        self.all_results = pd.concat([self.all_results, results_df], ignore_index=True)\n",
       "        self.save_results()\n",
       "    \n",
       "    def get_comparison_data(self):\n",
       "        if self.all_results.empty:\n",
       "            return pd.DataFrame()\n",
       "        \n",
       "        comparison_df = self.all_results.groupby(['Ferramenta', 'Categoria']).agg({\n",
       "            'Pontua√ß√£o': 'sum',\n",
       "            'Score': 'first'\n",
       "        }).reset_index()\n",
       "        \n",
       "        pivot_df = comparison_df.pivot(index='Ferramenta', columns='Categoria', values='Score')\n",
       "        total_scores = self.all_results.groupby('Ferramenta')['Pontua√ß√£o'].sum()\n",
       "        pivot_df['Pontua√ß√£o Total'] = total_scores\n",
       "        \n",
       "        return pivot_df.sort_values('Pontua√ß√£o Total', ascending=False)\n",
       "    \n",
       "    def plot_comparison_radar(self):\n",
       "        comparison_data = self.get_comparison_data()\n",
       "        if comparison_data.empty:\n",
       "            print(\"Nenhum dado dispon√≠vel para compara√ß√£o\")\n",
       "            return\n",
       "        \n",
       "        categories = list(questions.keys())\n",
       "        N = len(categories)\n",
       "        angles = [n / float(N) * 2 * pi for n in range(N)]\n",
       "        angles += angles[:1]\n",
       "        \n",
       "        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
       "        \n",
       "        tools = comparison_data.index.tolist()\n",
       "        for idx, tool in enumerate(tools):\n",
       "            values = comparison_data.loc[tool, categories].values.flatten().tolist()\n",
       "            values += values[:1]\n",
       "            ax.plot(angles, values, color=palette[idx % len(palette)], linewidth=2, \n",
       "                    linestyle='solid', label=f\"{tool} ({int(comparison_data.loc[tool, 'Pontua√ß√£o Total'])} pts)\")\n",
       "            ax.fill(angles, values, color=palette[idx % len(palette)], alpha=0.1)\n",
       "        \n",
       "        plt.xticks(angles[:-1], categories, color='grey', size=12)\n",
       "        ax.set_rlabel_position(30)\n",
       "        plt.yticks([20, 40, 60, 80, 100], [\"20%\", \"40%\", \"60%\", \"80%\", \"100%\"], color=\"grey\", size=10)\n",
       "        plt.ylim(0, 100)\n",
       "        \n",
       "        plt.title(\"Compara√ß√£o entre Ferramentas AutoML\\n(Score Normalizado por Categoria)\", size=15, y=1.1)\n",
       "        plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
       "        plt.show()\n",
       "    \n",
       "    def plot_comparison_bars(self):\n",
       "        comparison_data = self.get_comparison_data()\n",
       "        if comparison_data.empty:\n",
       "            print(\"Nenhum dado dispon√≠vel para compara√ß√£o\")\n",
       "            return\n",
       "        \n",
       "        categories = list(questions.keys())\n",
       "        tools = comparison_data.index.tolist()\n",
       "        bar_width = 0.15\n",
       "        index = np.arange(len(categories))\n",
       "        \n",
       "        plt.figure(figsize=(15, 8))\n",
       "        \n",
       "        for i, tool in enumerate(tools):\n",
       "            values = comparison_data.loc[tool, categories].values\n",
       "            plt.bar(index + i*bar_width, values, bar_width, \n",
       "                    color=palette[i % len(palette)], label=f\"{tool} ({int(comparison_data.loc[tool, 'Pontua√ß√£o Total'])} pts)\")\n",
       "        \n",
       "        plt.xlabel('Categorias')\n",
       "        plt.ylabel('Score Normalizado (%)')\n",
       "        plt.title('Compara√ß√£o de Ferramentas por Categoria')\n",
       "        plt.xticks(index + bar_width*(len(tools)-1)/2, categories, rotation=45)\n",
       "        plt.ylim(0, 110)\n",
       "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
       "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
       "        plt.tight_layout()\n",
       "        plt.show()\n",
       "\n",
       "class ToolEvaluator:\n",
       "    def __init__(self, comparative_system, tool_name):\n",
       "        self.comparative_system = comparative_system\n",
       "        self.tool_name = tool_name\n",
       "        self.results = pd.DataFrame(columns=['Categoria', 'Pergunta', 'N', 'Pontua√ß√£o', 'Score'])\n",
       "        self.W = 2\n",
       "        \n",
       "        self.create_widgets()\n",
       "    \n",
       "    def create_widgets(self):\n",
       "        self.widgets = {}\n",
       "        \n",
       "        for category, q_list in questions.items():\n",
       "            for question in q_list:\n",
       "                key = f\"{category} | {question}\"\n",
       "                self.widgets[key] = widgets.Dropdown(\n",
       "                    options=[\n",
       "                        ('N√£o atende (N=0)', 0),\n",
       "                        ('Parcial (N=1)', 1),\n",
       "                        ('Total (N‚â•2)', 2)\n",
       "                    ],\n",
       "                    description=question,\n",
       "                    style={'description_width': 'initial'},\n",
       "                    layout={'width': '800px'}\n",
       "                )\n",
       "        \n",
       "        self.submit_button = widgets.Button(\n",
       "            description=\"Salvar Avalia√ß√£o\",\n",
       "            button_style='success',\n",
       "            icon='save'\n",
       "        )\n",
       "        self.submit_button.on_click(self.save_evaluation)\n",
       "    \n",
       "    def display_interface(self):\n",
       "        display(widgets.HTML(f\"<h2 style='color:{primary_color};'>Avalia√ß√£o: {self.tool_name}</h2>\"))\n",
       "        \n",
       "        accordion = widgets.Accordion(children=[\n",
       "            widgets.VBox([self.widgets[key] for key in self.widgets if key.startswith(category)])\n",
       "            for category in questions.keys()\n",
       "        ])\n",
       "        \n",
       "        for i, category in enumerate(questions.keys()):\n",
       "            accordion.set_title(i, f\"{category} ({len(questions[category])} perguntas)\")\n",
       "        \n",
       "        display(accordion)\n",
       "        display(self.submit_button)\n",
       "    \n",
       "    def save_evaluation(self, b):\n",
       "        self.results = pd.DataFrame(columns=['Categoria', 'Pergunta', 'N', 'Pontua√ß√£o', 'Score'])\n",
       "        \n",
       "        for key, widget in self.widgets.items():\n",
       "            category, question = key.split(\" | \")\n",
       "            N = widget.value\n",
       "            points = min(N, 2)\n",
       "            \n",
       "            self.results.loc[len(self.results)] = {\n",
       "                'Categoria': category,\n",
       "                'Pergunta': question,\n",
       "                'N': N,\n",
       "                'Pontua√ß√£o': points,\n",
       "                'Score': None\n",
       "            }\n",
       "        \n",
       "        for category in questions.keys():\n",
       "            cat_data = self.results[self.results['Categoria'] == category]\n",
       "            sum_Pi = cat_data['Pontua√ß√£o'].sum()\n",
       "            N_questions = len(cat_data)\n",
       "            \n",
       "            S = (sum_Pi / (N_questions * self.W)) * 100 if N_questions > 0 else 0\n",
       "            \n",
       "            self.results.loc[self.results['Categoria'] == category, 'Score'] = S\n",
       "        \n",
       "        self.comparative_system.add_evaluation(self.tool_name, self.results)\n",
       "        \n",
       "        clear_output()\n",
       "        display(widgets.HTML(f\"<div style='color:green; font-weight:bold;'>Avalia√ß√£o para {self.tool_name} salva com sucesso!</div>\"))\n",
       "        self.display_results()\n",
       "    \n",
       "    def display_results(self):\n",
       "        display(widgets.HTML(f\"<h3>Resumo para {self.tool_name}</h3>\"))\n",
       "        \n",
       "        summary = self.results.groupby('Categoria').agg({\n",
       "            'Pontua√ß√£o': 'sum',\n",
       "            'Score': 'first'\n",
       "        })\n",
       "        summary['Perguntas'] = self.results.groupby('Categoria').size()\n",
       "        display(summary)\n",
       "        \n",
       "        display(widgets.HTML(f\"<p><b>Pontua√ß√£o Total:</b> {self.results['Pontua√ß√£o'].sum()} pontos</p>\"))\n",
       "\n",
       "class AutoMLEvaluationUI:\n",
       "    def __init__(self):\n",
       "        self.comparative_system = AutoMLComparativeEvaluation()\n",
       "        self.create_ui()\n",
       "    \n",
       "    def create_ui(self):\n",
       "        self.tool_selector = widgets.Dropdown(\n",
       "            options=['AutoSklearn', 'AutoGloun', 'TPOT', 'LinghtAutoML', 'MlJar', \n",
       "                    'AutoPyTorch', 'HyperGBM', 'MH-AutoML', 'Nova Ferramenta'],\n",
       "            description='Ferramenta:',\n",
       "            style={'description_width': 'initial'}\n",
       "        )\n",
       "        \n",
       "        self.new_tool_text = widgets.Text(\n",
       "            placeholder=\"Digite o nome de uma nova ferramenta\",\n",
       "            description='Outra:',\n",
       "            disabled=False\n",
       "        )\n",
       "        \n",
       "        self.start_button = widgets.Button(\n",
       "            description=\"Iniciar Avalia√ß√£o\",\n",
       "            button_style='primary',\n",
       "            icon='play'\n",
       "        )\n",
       "        \n",
       "        self.compare_button = widgets.Button(\n",
       "            description=\"Comparar Ferramentas\",\n",
       "            button_style='info',\n",
       "            icon='bar-chart'\n",
       "        )\n",
       "        \n",
       "        self.output = widgets.Output()\n",
       "        \n",
       "        self.start_button.on_click(self.start_evaluation)\n",
       "        self.compare_button.on_click(self.show_comparison)\n",
       "        \n",
       "        tools_box = widgets.HBox([self.tool_selector, self.new_tool_text])\n",
       "        buttons_box = widgets.HBox([self.start_button, self.compare_button])\n",
       "        \n",
       "        self.ui = widgets.VBox([\n",
       "            widgets.HTML(\"<h1 style='text-align: center; color: #1f77b4;'>Sistema de Avalia√ß√£o Comparativa de AutoML</h1>\"),\n",
       "            widgets.HTML(\"\"\"\n",
       "            <div style='background-color: #f7f7f7; padding: 15px; border-radius: 5px; margin-bottom: 20px;'>\n",
       "                <h3 style='color: #333;'>Como usar:</h3>\n",
       "                <ol>\n",
       "                    <li>Selecione uma ferramenta ou digite o nome de uma nova</li>\n",
       "                    <li>Clique em <b>Iniciar Avalia√ß√£o</b> para avaliar a ferramenta</li>\n",
       "                    <li>Para cada crit√©rio, indique <b>N</b> (n√∫mero de maneiras que a ferramenta atende):</li>\n",
       "                    <ul>\n",
       "                        <li><b>N=0</b>: N√£o atende (0 pontos)</li>\n",
       "                        <li><b>N=1</b>: Atende parcialmente (1 ponto)</li>\n",
       "                        <li><b>N‚â•2</b>: Atende totalmente (2 pontos)</li>\n",
       "                    </ul>\n",
       "                    <li>Clique em <b>Comparar Ferramentas</b> para ver gr√°ficos comparativos</li>\n",
       "                </ol>\n",
       "            </div>\n",
       "            \"\"\"),\n",
       "            tools_box,\n",
       "            buttons_box,\n",
       "            self.output\n",
       "        ])\n",
       "    \n",
       "    def display(self):\n",
       "        display(self.ui)\n",
       "    \n",
       "    def start_evaluation(self, b):\n",
       "        with self.output:\n",
       "            clear_output()\n",
       "            tool_name = self.new_tool_text.value if self.new_tool_text.value else self.tool_selector.value\n",
       "            if not tool_name:\n",
       "                print(\"Por favor, selecione ou digite o nome de uma ferramenta\")\n",
       "                return\n",
       "                \n",
       "            evaluator = ToolEvaluator(self.comparative_system, tool_name)\n",
       "            evaluator.display_interface()\n",
       "    \n",
       "    def show_comparison(self, b):\n",
       "        with self.output:\n",
       "            clear_output()\n",
       "            if self.comparative_system.all_results.empty:\n",
       "                print(\"Nenhuma ferramenta avaliada ainda. Por favor, avalie pelo menos uma ferramenta.\")\n",
       "                return\n",
       "                \n",
       "            display(widgets.HTML(\"<h2 style='color:#1f77b4;'>Compara√ß√£o entre Ferramentas</h2>\"))\n",
       "            \n",
       "            comparison_data = self.comparative_system.get_comparison_data()\n",
       "            display(comparison_data)\n",
       "            \n",
       "            self.comparative_system.plot_comparison_radar()\n",
       "            self.comparative_system.plot_comparison_bars()\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "// Mant√©m os outputs vis√≠veis\n",
    "Jupyter.notebook.get_cells().forEach(function(cell) {\n",
    "    if (cell.cell_type === 'code') {\n",
    "        cell.output_area.outputs = cell.output_area.outputs || [];\n",
    "    }\n",
    "});\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import pi\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Configura√ß√µes de estilo\n",
    "plt.style.use('seaborn')\n",
    "primary_color = '#1f77b4'\n",
    "palette = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "\n",
    "## Dados das Perguntas por Categoria\n",
    "questions = {\n",
    "    \"Descri√ß√£o Funcional\": [\n",
    "        \"√â poss√≠vel identificar quais s√£o as etapas do pipeline?\",\n",
    "        \"√â poss√≠vel identificar os componentes utilizados em cada etapa?\",\n",
    "        \"Est√° claro quais s√£o as entradas e sa√≠das?\",\n",
    "        \"Est√° claro como o processo √© executado na pr√°tica?\"\n",
    "    ],\n",
    "    \"An√°lise Estat√≠stica\": [\n",
    "        \"As previs√µes do modelo s√£o apresentadas para as classes (malwares e benignos)?\",\n",
    "        \"Mantem o hist√≥rico dos dados utilizados para treinar e testar o modelo?\",\n",
    "        \"Na sele√ß√£o de caracter√≠sticas √© informado ao usuario quais s√£o as mais relevantes?\",\n",
    "        \"As m√©tricas de desempenho do modelo s√£o apresentadas?\",\n",
    "        \"Existem formas de visualizar os resultados (gr√°ficos, tabelas)?\"\n",
    "    ],\n",
    "    \"Transpar√™ncia Algor√≠tmica\": [\n",
    "        \"√â poss√≠vel identificar quais modelos s√£o utilizados?\",\n",
    "        \"√â possivel identificar avisos, informa√ß√µes e erros registrados nos logs do sistema?\",\n",
    "        \"√â poss√≠vel identificar os hiperparametros dos modelos gerados?\",\n",
    "        \"√â poss√≠vel identificar se os dados s√£o balanceados?\"\n",
    "    ],\n",
    "    \"Interpretabilidade\": [\n",
    "        \"O motivo da redu√ß√£o da dimensionalidade √© explicado? (Pr√©-Modelo)\",\n",
    "        \"Existem t√©cnicas espec√≠ficas de redu√ß√£o de dimensionalidade para o dom√≠nio? (Pr√©-Modelo)\",\n",
    "        \"H√° m√©todos de interpretabilidade global dispon√≠veis?\",\n",
    "        \"O framework oferece modelos com interpretabilidade intr√≠nseca? (In-Modelo)\",\n",
    "        \"√â poss√≠vel avaliar a diferen√ßa entre previs√µes e valores reais?\",\n",
    "        \"H√° m√©todos de interpretabilidade independentes do modelo? (P√≥s-Modelo)\"\n",
    "    ],\n",
    "    \"An√°lise Interna\": [\n",
    "        \"Existem m√©todos para visualizar a estrutura do modelo? (P√≥s-Modelo)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Arquivo para salvar os resultados\n",
    "RESULTS_FILE = 'automl_evaluations.pkl'\n",
    "\n",
    "class AutoMLComparativeEvaluation:\n",
    "    def __init__(self):\n",
    "        self.all_results = pd.DataFrame()\n",
    "        self.load_existing_data()\n",
    "    \n",
    "    def load_existing_data(self):\n",
    "        if os.path.exists(RESULTS_FILE):\n",
    "            with open(RESULTS_FILE, 'rb') as f:\n",
    "                self.all_results = pickle.load(f)\n",
    "    \n",
    "    def save_results(self):\n",
    "        with open(RESULTS_FILE, 'wb') as f:\n",
    "            pickle.dump(self.all_results, f)\n",
    "    \n",
    "    def add_evaluation(self, tool_name, results_df):\n",
    "        results_df['Ferramenta'] = tool_name\n",
    "        self.all_results = pd.concat([self.all_results, results_df], ignore_index=True)\n",
    "        self.save_results()\n",
    "    \n",
    "    def get_comparison_data(self):\n",
    "        if self.all_results.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        comparison_df = self.all_results.groupby(['Ferramenta', 'Categoria']).agg({\n",
    "            'Pontua√ß√£o': 'sum',\n",
    "            'Score': 'first'\n",
    "        }).reset_index()\n",
    "        \n",
    "        pivot_df = comparison_df.pivot(index='Ferramenta', columns='Categoria', values='Score')\n",
    "        total_scores = self.all_results.groupby('Ferramenta')['Pontua√ß√£o'].sum()\n",
    "        pivot_df['Pontua√ß√£o Total'] = total_scores\n",
    "        \n",
    "        return pivot_df.sort_values('Pontua√ß√£o Total', ascending=False)\n",
    "    \n",
    "    def plot_comparison_radar(self):\n",
    "        comparison_data = self.get_comparison_data()\n",
    "        if comparison_data.empty:\n",
    "            print(\"Nenhum dado dispon√≠vel para compara√ß√£o\")\n",
    "            return\n",
    "        \n",
    "        categories = list(questions.keys())\n",
    "        N = len(categories)\n",
    "        angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "        angles += angles[:1]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "        \n",
    "        tools = comparison_data.index.tolist()\n",
    "        for idx, tool in enumerate(tools):\n",
    "            values = comparison_data.loc[tool, categories].values.flatten().tolist()\n",
    "            values += values[:1]\n",
    "            ax.plot(angles, values, color=palette[idx % len(palette)], linewidth=2, \n",
    "                    linestyle='solid', label=f\"{tool} ({int(comparison_data.loc[tool, 'Pontua√ß√£o Total'])} pts)\")\n",
    "            ax.fill(angles, values, color=palette[idx % len(palette)], alpha=0.1)\n",
    "        \n",
    "        plt.xticks(angles[:-1], categories, color='grey', size=12)\n",
    "        ax.set_rlabel_position(30)\n",
    "        plt.yticks([20, 40, 60, 80, 100], [\"20%\", \"40%\", \"60%\", \"80%\", \"100%\"], color=\"grey\", size=10)\n",
    "        plt.ylim(0, 100)\n",
    "        \n",
    "        plt.title(\"Compara√ß√£o entre Ferramentas AutoML\\n(Score Normalizado por Categoria)\", size=15, y=1.1)\n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_comparison_bars(self):\n",
    "        comparison_data = self.get_comparison_data()\n",
    "        if comparison_data.empty:\n",
    "            print(\"Nenhum dado dispon√≠vel para compara√ß√£o\")\n",
    "            return\n",
    "        \n",
    "        categories = list(questions.keys())\n",
    "        tools = comparison_data.index.tolist()\n",
    "        bar_width = 0.15\n",
    "        index = np.arange(len(categories))\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        for i, tool in enumerate(tools):\n",
    "            values = comparison_data.loc[tool, categories].values\n",
    "            plt.bar(index + i*bar_width, values, bar_width, \n",
    "                    color=palette[i % len(palette)], label=f\"{tool} ({int(comparison_data.loc[tool, 'Pontua√ß√£o Total'])} pts)\")\n",
    "        \n",
    "        plt.xlabel('Categorias')\n",
    "        plt.ylabel('Score Normalizado (%)')\n",
    "        plt.title('Compara√ß√£o de Ferramentas por Categoria')\n",
    "        plt.xticks(index + bar_width*(len(tools)-1)/2, categories, rotation=45)\n",
    "        plt.ylim(0, 110)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "class ToolEvaluator:\n",
    "    def __init__(self, comparative_system, tool_name):\n",
    "        self.comparative_system = comparative_system\n",
    "        self.tool_name = tool_name\n",
    "        self.results = pd.DataFrame(columns=['Categoria', 'Pergunta', 'N', 'Pontua√ß√£o', 'Score'])\n",
    "        self.W = 2\n",
    "        \n",
    "        self.create_widgets()\n",
    "    \n",
    "    def create_widgets(self):\n",
    "        self.widgets = {}\n",
    "        \n",
    "        for category, q_list in questions.items():\n",
    "            for question in q_list:\n",
    "                key = f\"{category} | {question}\"\n",
    "                self.widgets[key] = widgets.Dropdown(\n",
    "                    options=[\n",
    "                        ('N√£o atende (N=0)', 0),\n",
    "                        ('Parcial (N=1)', 1),\n",
    "                        ('Total (N‚â•2)', 2)\n",
    "                    ],\n",
    "                    description=question,\n",
    "                    style={'description_width': 'initial'},\n",
    "                    layout={'width': '800px'}\n",
    "                )\n",
    "        \n",
    "        self.submit_button = widgets.Button(\n",
    "            description=\"Salvar Avalia√ß√£o\",\n",
    "            button_style='success',\n",
    "            icon='save'\n",
    "        )\n",
    "        self.submit_button.on_click(self.save_evaluation)\n",
    "    \n",
    "    def display_interface(self):\n",
    "        display(widgets.HTML(f\"<h2 style='color:{primary_color};'>Avalia√ß√£o: {self.tool_name}</h2>\"))\n",
    "        \n",
    "        accordion = widgets.Accordion(children=[\n",
    "            widgets.VBox([self.widgets[key] for key in self.widgets if key.startswith(category)])\n",
    "            for category in questions.keys()\n",
    "        ])\n",
    "        \n",
    "        for i, category in enumerate(questions.keys()):\n",
    "            accordion.set_title(i, f\"{category} ({len(questions[category])} perguntas)\")\n",
    "        \n",
    "        display(accordion)\n",
    "        display(self.submit_button)\n",
    "    \n",
    "    def save_evaluation(self, b):\n",
    "        self.results = pd.DataFrame(columns=['Categoria', 'Pergunta', 'N', 'Pontua√ß√£o', 'Score'])\n",
    "        \n",
    "        for key, widget in self.widgets.items():\n",
    "            category, question = key.split(\" | \")\n",
    "            N = widget.value\n",
    "            points = min(N, 2)\n",
    "            \n",
    "            self.results.loc[len(self.results)] = {\n",
    "                'Categoria': category,\n",
    "                'Pergunta': question,\n",
    "                'N': N,\n",
    "                'Pontua√ß√£o': points,\n",
    "                'Score': None\n",
    "            }\n",
    "        \n",
    "        for category in questions.keys():\n",
    "            cat_data = self.results[self.results['Categoria'] == category]\n",
    "            sum_Pi = cat_data['Pontua√ß√£o'].sum()\n",
    "            N_questions = len(cat_data)\n",
    "            \n",
    "            S = (sum_Pi / (N_questions * self.W)) * 100 if N_questions > 0 else 0\n",
    "            \n",
    "            self.results.loc[self.results['Categoria'] == category, 'Score'] = S\n",
    "        \n",
    "        self.comparative_system.add_evaluation(self.tool_name, self.results)\n",
    "        \n",
    "        clear_output()\n",
    "        display(widgets.HTML(f\"<div style='color:green; font-weight:bold;'>Avalia√ß√£o para {self.tool_name} salva com sucesso!</div>\"))\n",
    "        self.display_results()\n",
    "    \n",
    "    def display_results(self):\n",
    "        display(widgets.HTML(f\"<h3>Resumo para {self.tool_name}</h3>\"))\n",
    "        \n",
    "        summary = self.results.groupby('Categoria').agg({\n",
    "            'Pontua√ß√£o': 'sum',\n",
    "            'Score': 'first'\n",
    "        })\n",
    "        summary['Perguntas'] = self.results.groupby('Categoria').size()\n",
    "        display(summary)\n",
    "        \n",
    "        display(widgets.HTML(f\"<p><b>Pontua√ß√£o Total:</b> {self.results['Pontua√ß√£o'].sum()} pontos</p>\"))\n",
    "\n",
    "class AutoMLEvaluationUI:\n",
    "    def __init__(self):\n",
    "        self.comparative_system = AutoMLComparativeEvaluation()\n",
    "        self.create_ui()\n",
    "    \n",
    "    def create_ui(self):\n",
    "        self.tool_selector = widgets.Dropdown(\n",
    "            options=['AutoSklearn', 'AutoGloun', 'TPOT', 'LinghtAutoML', 'MlJar', \n",
    "                    'AutoPyTorch', 'HyperGBM', 'MH-AutoML', 'Nova Ferramenta'],\n",
    "            description='Ferramenta:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.new_tool_text = widgets.Text(\n",
    "            placeholder=\"Digite o nome de uma nova ferramenta\",\n",
    "            description='Outra:',\n",
    "            disabled=False\n",
    "        )\n",
    "        \n",
    "        self.start_button = widgets.Button(\n",
    "            description=\"Iniciar Avalia√ß√£o\",\n",
    "            button_style='primary',\n",
    "            icon='play'\n",
    "        )\n",
    "        \n",
    "        self.compare_button = widgets.Button(\n",
    "            description=\"Comparar Ferramentas\",\n",
    "            button_style='info',\n",
    "            icon='bar-chart'\n",
    "        )\n",
    "        \n",
    "        self.output = widgets.Output()\n",
    "        \n",
    "        self.start_button.on_click(self.start_evaluation)\n",
    "        self.compare_button.on_click(self.show_comparison)\n",
    "        \n",
    "        tools_box = widgets.HBox([self.tool_selector, self.new_tool_text])\n",
    "        buttons_box = widgets.HBox([self.start_button, self.compare_button])\n",
    "        \n",
    "        self.ui = widgets.VBox([\n",
    "            widgets.HTML(\"<h1 style='text-align: center; color: #1f77b4;'>Sistema de Avalia√ß√£o Comparativa de AutoML</h1>\"),\n",
    "            widgets.HTML(\"\"\"\n",
    "            <div style='background-color: #f7f7f7; padding: 15px; border-radius: 5px; margin-bottom: 20px;'>\n",
    "                <h3 style='color: #333;'>Como usar:</h3>\n",
    "                <ol>\n",
    "                    <li>Selecione uma ferramenta ou digite o nome de uma nova</li>\n",
    "                    <li>Clique em <b>Iniciar Avalia√ß√£o</b> para avaliar a ferramenta</li>\n",
    "                    <li>Para cada crit√©rio, indique <b>N</b> (n√∫mero de maneiras que a ferramenta atende):</li>\n",
    "                    <ul>\n",
    "                        <li><b>N=0</b>: N√£o atende (0 pontos)</li>\n",
    "                        <li><b>N=1</b>: Atende parcialmente (1 ponto)</li>\n",
    "                        <li><b>N‚â•2</b>: Atende totalmente (2 pontos)</li>\n",
    "                    </ul>\n",
    "                    <li>Clique em <b>Comparar Ferramentas</b> para ver gr√°ficos comparativos</li>\n",
    "                </ol>\n",
    "            </div>\n",
    "            \"\"\"),\n",
    "            tools_box,\n",
    "            buttons_box,\n",
    "            self.output\n",
    "        ])\n",
    "    \n",
    "    def display(self):\n",
    "        display(self.ui)\n",
    "    \n",
    "    def start_evaluation(self, b):\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "            tool_name = self.new_tool_text.value if self.new_tool_text.value else self.tool_selector.value\n",
    "            if not tool_name:\n",
    "                print(\"Por favor, selecione ou digite o nome de uma ferramenta\")\n",
    "                return\n",
    "                \n",
    "            evaluator = ToolEvaluator(self.comparative_system, tool_name)\n",
    "            evaluator.display_interface()\n",
    "    \n",
    "    def show_comparison(self, b):\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "            if self.comparative_system.all_results.empty:\n",
    "                print(\"Nenhuma ferramenta avaliada ainda. Por favor, avalie pelo menos uma ferramenta.\")\n",
    "                return\n",
    "                \n",
    "            display(widgets.HTML(\"<h2 style='color:#1f77b4;'>Compara√ß√£o entre Ferramentas</h2>\"))\n",
    "            \n",
    "            comparison_data = self.comparative_system.get_comparison_data()\n",
    "            display(comparison_data)\n",
    "            \n",
    "            self.comparative_system.plot_comparison_radar()\n",
    "            self.comparative_system.plot_comparison_bars()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdc8c95d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1740cc3c647470d8ce5a73b5c86904f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h1 style='text-align: center; color: #1f77b4;'>Sistema de Avalia√ß√£o Comparativa de‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Iniciar a interface\n",
    "ui = AutoMLEvaluationUI()\n",
    "ui.display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
