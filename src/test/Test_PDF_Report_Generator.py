#!/usr/bin/env python3
"""
Teste completo do gerador de relat√≥rios PDF do MH-AutoML
Testa todas as funcionalidades: convers√£o HTML->PNG, gera√ß√£o PDF, fallbacks
"""

import os
import sys
import pandas as pd
import numpy as np
import unittest
from unittest.mock import Mock, patch

# Adicionar o diret√≥rio pai ao path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class TestPDFReportGenerator(unittest.TestCase):
    """Testes para o gerador de relat√≥rios PDF"""
    
    def setUp(self):
        """Configura√ß√£o inicial para cada teste"""
        self.test_results_folder = "test_results"
        if not os.path.exists(self.test_results_folder):
            os.makedirs(self.test_results_folder)
        
        # Criar dados de teste
        np.random.seed(42)
        self.n_samples = 50
        self.n_features = 5
        self.X = np.random.randn(self.n_samples, self.n_features)
        self.y = np.random.randint(0, 2, self.n_samples)
        
        # DataFrame simulado
        self.feature_names = [f'feature_{i}' for i in range(self.n_features)]
        self.dataset_df = pd.DataFrame(self.X, columns=self.feature_names)
        self.dataset_df['target'] = self.y
        
        # Criar arquivos de teste
        self._create_test_files()
    
    def _create_test_files(self):
        """Criar arquivos de teste seguindo o template MLflow"""
        import matplotlib.pyplot as plt
        
        # Arquivos de pr√©-processamento
        fig, ax = plt.subplots(figsize=(8, 6))
        data = np.random.rand(10, 10)
        im = ax.imshow(data, cmap='viridis')
        ax.set_title('Missing Values Heatmap')
        plt.colorbar(im)
        plt.tight_layout()
        plt.savefig(os.path.join(self.test_results_folder, "missing_values_heatmap.png"), dpi=100, bbox_inches='tight')
        plt.close()
        
        # clean_missing_values_heatmap.png
        fig, ax = plt.subplots(figsize=(8, 6))
        data = np.random.rand(10, 10)
        im = ax.imshow(data, cmap='plasma')
        ax.set_title('Clean Missing Values Heatmap')
        plt.colorbar(im)
        plt.tight_layout()
        plt.savefig(os.path.join(self.test_results_folder, "clean_missing_values_heatmap.png"), dpi=100, bbox_inches='tight')
        plt.close()
        
        # lasso_feature_importance.png
        fig, ax = plt.subplots(figsize=(8, 6))
        features = self.feature_names[:3]
        importance = np.random.rand(len(features))
        ax.barh(features, importance)
        ax.set_title('LASSO Feature Importance')
        plt.tight_layout()
        plt.savefig(os.path.join(self.test_results_folder, "lasso_feature_importance.png"), dpi=100, bbox_inches='tight')
        plt.close()
        
        # optuna_optimization_history.png
        fig, ax = plt.subplots(figsize=(8, 6))
        trials = range(1, 21)
        scores = np.random.rand(20)
        ax.plot(trials, scores)
        ax.set_title('Optuna Optimization History')
        ax.set_xlabel('Trial')
        ax.set_ylabel('Score')
        plt.tight_layout()
        plt.savefig(os.path.join(self.test_results_folder, "optuna_optimization_history.png"), dpi=100, bbox_inches='tight')
        plt.close()
        
        # performance_metrics.jpg
        fig, ax = plt.subplots(figsize=(8, 6))
        metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
        values = np.random.rand(4)
        ax.bar(metrics, values)
        ax.set_title('Performance Metrics')
        plt.tight_layout()
        plt.savefig(os.path.join(self.test_results_folder, "performance_metrics.jpg"), dpi=100, bbox_inches='tight')
        plt.close()
        
        # Arquivo HTML de teste
        html_content = """
        <!DOCTYPE html>
        <html>
        <head><title>Test HTML</title></head>
        <body>
            <h1>Test HTML Content</h1>
            <p>This is a test HTML file for conversion.</p>
        </body>
        </html>
        """
        with open(os.path.join(self.test_results_folder, "test_plot.html"), 'w') as f:
            f.write(html_content)
    
    def test_pdf_generator_initialization(self):
        """Testa inicializa√ß√£o do gerador de PDF"""
        from model.tools.pdf_report_generator import PDFReportGenerator
        
        generator = PDFReportGenerator(self.test_results_folder)
        self.assertIsNotNone(generator)
        self.assertEqual(generator.results_folder, self.test_results_folder)
    
    def test_html_to_png_conversion_methods(self):
        """Testa m√©todos de convers√£o HTML para PNG"""
        from model.tools.pdf_report_generator import PDFReportGenerator
        
        generator = PDFReportGenerator(self.test_results_folder)
        html_file = os.path.join(self.test_results_folder, "test_plot.html")
        
        # Testar convers√£o com Playwright (se dispon√≠vel)
        try:
            png_path = generator._convert_html_to_png_playwright(html_file)
            if png_path:
                self.assertTrue(os.path.exists(png_path))
        except Exception:
            pass  # Playwright pode n√£o estar dispon√≠vel
        
        # Testar convers√£o com Selenium (se dispon√≠vel)
        try:
            png_path = generator._convert_html_to_png_selenium(html_file)
            if png_path:
                self.assertTrue(os.path.exists(png_path))
        except Exception:
            pass  # Selenium pode n√£o estar dispon√≠vel
    
    def test_pdf_generation_methods(self):
        """Testa m√©todos de gera√ß√£o de PDF"""
        from model.tools.pdf_report_generator import PDFReportGenerator
        
        generator = PDFReportGenerator(self.test_results_folder)
        html_content = "<html><body><h1>Test</h1></body></html>"
        
        # Testar WeasyPrint (se dispon√≠vel)
        try:
            pdf_path = generator._generate_pdf_weasyprint(html_content)
            if pdf_path:
                self.assertTrue(os.path.exists(pdf_path))
        except Exception:
            pass  # WeasyPrint pode n√£o estar dispon√≠vel
        
        # Testar pdfkit (se dispon√≠vel)
        try:
            pdf_path = generator._generate_pdf_pdfkit(html_content)
            if pdf_path:
                self.assertTrue(os.path.exists(pdf_path))
        except Exception:
            pass  # pdfkit pode n√£o estar dispon√≠vel
    
    def test_reportlab_fallback(self):
        """Testa gera√ß√£o de PDF com ReportLab (fallback)"""
        from model.tools.pdf_report_generator import PDFReportGenerator
        
        generator = PDFReportGenerator(self.test_results_folder)
        
        # Mock data
        class MockPipeline:
            def __init__(self):
                self.steps = [
                    ('Preprocessor', Mock()),
                    ('Feature engineering', Mock()),
                    ('Ensemble Classifier', Mock())
                ]
        
        class MockModel:
            def __init__(self):
                self.estimators_ = {}
                self.named_estimators_ = {}
            
            def get_params(self):
                return {'param1': 'value1'}
        
        # Gerar PDF
        pdf_filename = generator.generate_pdf_report(
            pipeline=MockPipeline(),
            display_data=Mock(),
            study=None,
            best_model=MockModel(),
            model_name="TestModel",
            model_params={'param1': 'value1'},
            select_results=None,
            report="Test Report",
            y_test=self.y[:20],
            y_pred=self.y[:20],
            feature_names=self.feature_names,
            feature_selection_info={'method': 'lasso'},
            shap_exp_filepath=None,
            exp_filepath=None,
            lime_exp_filepath=None
        )
        
        self.assertIsNotNone(pdf_filename)
        self.assertTrue(os.path.exists(pdf_filename))
        self.assertTrue(pdf_filename.endswith('.pdf'))
        
        # Verificar tamanho do arquivo
        file_size = os.path.getsize(pdf_filename)
        self.assertGreater(file_size, 1000)  # Deve ter pelo menos 1KB
    
    def test_template_structure(self):
        """Testa se o PDF segue o template correto do MH-AutoML"""
        from model.tools.pdf_report_generator import PDFReportGenerator
        
        generator = PDFReportGenerator(self.test_results_folder)
        
        # Mock data
        class MockPipeline:
            def __init__(self):
                self.steps = [
                    ('Preprocessor', Mock()),
                    ('Feature engineering', Mock()),
                    ('Ensemble Classifier', Mock())
                ]
        
        class MockModel:
            def __init__(self):
                self.estimators_ = {}
                self.named_estimators_ = {}
            
            def get_params(self):
                return {'param1': 'value1'}
        
        # Gerar PDF
        pdf_filename = generator.generate_pdf_report(
            pipeline=MockPipeline(),
            display_data=Mock(),
            study=None,
            best_model=MockModel(),
            model_name="TestModel",
            model_params={'param1': 'value1'},
            select_results=None,
            report="Test Report",
            y_test=self.y[:20],
            y_pred=self.y[:20],
            feature_names=self.feature_names,
            feature_selection_info={'method': 'lasso'},
            shap_exp_filepath=None,
            exp_filepath=None,
            lime_exp_filepath=None
        )
        
        # Verificar se o arquivo foi gerado
        self.assertIsNotNone(pdf_filename)
        self.assertTrue(os.path.exists(pdf_filename))
        
        # Verificar se √© PDF (n√£o HTML fallback)
        if pdf_filename.endswith('.pdf'):
            print("‚úÖ PDF gerado corretamente com template MH-AutoML")
        else:
            print("üìÑ HTML gerado como fallback")
    
    def test_artifact_categorization(self):
        """Testa categoriza√ß√£o de artefatos seguindo template MLflow"""
        from model.tools.pdf_report_generator import PDFReportGenerator
        
        generator = PDFReportGenerator(self.test_results_folder)
        
        # Verificar se os arquivos foram categorizados corretamente
        expected_categories = [
            "00_Data_info",
            "01_preprocessing", 
            "02_feature_engineering",
            "03_model_optimization",
            "04_evaluation_metrics",
            "05_interpretability",
            "Reports"
        ]
        
        # Simular categoriza√ß√£o
        artifact_categories = {
            "00_Data_info": [],
            "01_preprocessing": ["missing_values_heatmap.png", "clean_missing_values_heatmap.png"],
            "02_feature_engineering": ["lasso_feature_importance.png"],
            "03_model_optimization": ["optuna_optimization_history.png"],
            "04_evaluation_metrics": ["performance_metrics.jpg"],
            "05_interpretability": [],
            "Reports": []
        }
        
        # Verificar se as categorias est√£o corretas
        for category in expected_categories:
            self.assertIn(category, artifact_categories)
    
    def tearDown(self):
        """Limpeza ap√≥s cada teste"""
        # Manter os arquivos de teste para inspe√ß√£o manual
        pass

def run_tests():
    """Executa todos os testes"""
    print("üß™ Executando testes do gerador de PDF...")
    
    # Criar suite de testes
    suite = unittest.TestLoader().loadTestsFromTestCase(TestPDFReportGenerator)
    
    # Executar testes
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # Resumo
    print(f"\nüìä Resumo dos testes:")
    print(f"   ‚úÖ Testes passaram: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"   ‚ùå Testes falharam: {len(result.failures)}")
    print(f"   üí• Testes com erro: {len(result.errors)}")
    print(f"   üìà Total: {result.testsRun}")
    
    return result.wasSuccessful()

if __name__ == "__main__":
    success = run_tests()
    if success:
        print("\nüéâ Todos os testes passaram!")
    else:
        print("\nüí• Alguns testes falharam.")
    
    print("\nüìö Para mais informa√ß√µes, consulte o PDF_REPORT_GUIDE.md") 